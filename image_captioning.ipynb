{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, RepeatVector, Concatenate, Activation\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_list(filename):\n",
    "    with open(filename,'r') as image_list_f: \n",
    "        return [line.strip() for line in image_list_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'dataset'\n",
    "train_list = load_image_list(os.path.join(folder, 'Flickr_8k.trainImages.txt'))\n",
    "dev_list = load_image_list(os.path.join(folder,'Flickr_8k.devImages.txt'))\n",
    "test_list = load_image_list(os.path.join(folder,'Flickr_8k.testImages.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_list), len(dev_list), len(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = os.path.join(folder, \"Flickr8k_Dataset\")\n",
    "image = PIL.Image.open(os.path.join(IMG_PATH, dev_list[100]))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"encodings\" \n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_name):\n",
    "    image = PIL.Image.open(os.path.join(IMG_PATH, image_name))\n",
    "    return np.asarray(image.resize((299,299))) / 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(dev_list[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model = InceptionV3(weights='imagenet')\n",
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = img_model.input\n",
    "new_output = img_model.layers[-2].output\n",
    "img_encoder = Model(new_input, new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess_img(image)\n",
    "    vec = model.predict(image)\n",
    "    vec = np.reshape(vec, (vec.shape[1]))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#run the encode function on all train images\n",
    "start = time.time()\n",
    "encoding_train = {}\n",
    "for img in train_img:\n",
    "    encoding_train[img[len(images):]] = encode(img)\n",
    "print(\"Time Taken is: \" + str(time.time() - start))\n",
    "\n",
    "#Encode all the test images\n",
    "start = time.time()\n",
    "encoding_test = {}\n",
    "for img in test_img:\n",
    "    encoding_test[img[len(images):]] = encode(img)\n",
    "print(\"Time taken is: \" + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = get_image(dev_list[25])\n",
    "encoded_image = img_encoder.predict(np.array([new_image]))\n",
    "encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_generator(img_list):\n",
    "    for img_file in img_list:\n",
    "        img = PIL.Image.open(os.path.join(\"dataset\", \"Flickr8k_Dataset\", img_file))\n",
    "        img = img.resize((299, 299))  # Resize the image to (299, 299)\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add an extra dimension for batch size\n",
    "        yield img_array\n",
    "        img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = img_encoder.predict_generator(img_generator(train_list), steps=len(train_list), verbose=1)\n",
    "enc_dev = img_encoder.predict_generator(img_generator(dev_list), steps=len(dev_list), verbose=1)\n",
    "enc_test = img_encoder.predict_generator(img_generator(test_list), steps=len(test_list), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"encodings\" \n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "\n",
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_train.npy\"), enc_train)\n",
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_dev.npy\"), enc_dev)\n",
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_test.npy\"), enc_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image captions\n",
    "\n",
    "def read_image_descriptions(filename):    \n",
    "    image_descriptions = defaultdict(list)   \n",
    "    \n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            image_name, image_description = line.strip().split('\\t')\n",
    "            #print(image_name[:-2], \"<START> \" + image_description.lower() + \" <END>\")\n",
    "            sequence = \"<START> \" + image_description.lower() + \" <END>\"\n",
    "            image_descriptions[image_name[:-2]].append(sequence.split())\n",
    "\n",
    "    return image_descriptions\n",
    "\n",
    "descriptions = read_image_descriptions(\"dataset/Flickr8k.token.txt\")\n",
    "print(descriptions[dev_list[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary that maps tokens in training data to numbers \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "id_to_word = defaultdict(str)\n",
    "word_to_id = defaultdict(int)\n",
    "\n",
    "train_set_tokens = set()\n",
    "\n",
    "for file in train_list:\n",
    "    for seq in descriptions[file]:\n",
    "        for word in seq:\n",
    "            train_set_tokens.add(word)\n",
    "\n",
    "train_set_tokens = list(train_set_tokens)\n",
    "train_set_tokens.sort()\n",
    "\n",
    "for i, word in enumerate(train_set_tokens):\n",
    "    id_to_word[i] = word\n",
    "    word_to_id[word] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_id[\"dog\"], id_to_word[1985])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_clean_descriptions(des, dataset):\n",
    "    dataset_des = dict()\n",
    "    for key, des_list in des.items():\n",
    "        if key in dataset:\n",
    "            if key not in dataset_des:\n",
    "                dataset_des[key] = list()\n",
    "            for line in des_list:\n",
    "                dataset_des[key].append(' '.join(line))\n",
    "    return dataset_des\n",
    "\n",
    "train_descriptions = load_clean_descriptions(descriptions, train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383454, 2048)\n"
     ]
    }
   ],
   "source": [
    "#data generation\n",
    "\n",
    "enc_train = np.load(os.path.join(OUTPUT_PATH,\"encoded_images_train.npy\"))\n",
    "enc_dev = np.load(os.path.join(OUTPUT_PATH,\"encoded_images_dev.npy\"))\n",
    "enc_test = np.load(os.path.join(OUTPUT_PATH,\"encoded_images_test.npy\"))\n",
    "\n",
    "train_features = defaultdict(list)\n",
    "\n",
    "for i, image_id in enumerate(train_list):\n",
    "    train_features[image_id].append(enc_train[i])\n",
    "\n",
    "\n",
    "X1, X2, y = list(), list(), list()\n",
    "MAX_LEN = 40\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "for key, des_list in train_descriptions.items():\n",
    "    pic = train_features[key][0]\n",
    "    \n",
    "    for cap in des_list:\n",
    "        seq = [word_to_id[word] for word in cap.split(' ') if word in word_to_id]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen = MAX_LEN)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "            #store\n",
    "            X1.append(pic)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "\n",
    "X2 = np.array(X2)\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "glove = open('./glove.6B.200d.txt', 'r', encoding = 'utf-8').read()\n",
    "for line in glove.split(\"\\n\"):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    indices = np.asarray(values[1: ], dtype = 'float32')\n",
    "    embeddings_index[word] = indices\n",
    "print('Total word vectors: ' + str(len(embeddings_index)))\n",
    "\n",
    "emb_dim = 200\n",
    "emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in word_to_id.items():\n",
    "    emb_vec = embeddings_index.get(word)\n",
    "    if emb_vec is not None:\n",
    "        emb_matrix[i] = emb_vec\n",
    "emb_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
    "from keras.layers import concatenate, BatchNormalization, Input\n",
    "from keras.layers import add\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "ip1 = Input(shape = (2048, ))\n",
    "fe1 = Dropout(0.2)(ip1)\n",
    "fe2 = Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = Input(shape = (MAX_LEN, ))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)\n",
    "se2 = Dropout(0.2)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model = Model(inputs = [ip1, ip2], outputs = outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(pic):\n",
    "    start = '<START>'\n",
    "    for i in range(MAX_LEN):\n",
    "        seq = [word_to_id[word] for word in start.split() if word in word_to_id]\n",
    "        seq = pad_sequences([seq], maxlen = MAX_LEN)\n",
    "        yhat = model.predict([pic, seq])\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = id_to_word[yhat]\n",
    "        start += ' ' + word\n",
    "        if word == '<END>':\n",
    "            break\n",
    "    final = start.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m glove \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./glove.6B.200d.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m glove\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     values \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     word \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vaibhavsourirajan/Documents/Programming/image_captioning/image_captioning.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(values[\u001b[39m1\u001b[39m: ], dtype \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
